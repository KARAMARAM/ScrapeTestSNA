import pandas as pd
import re
from datetime import datetime
import os

def manual_ner_analysis(articles_path="Articles/cleaned_articles_combined.csv", output_path="manual_ner_analysis_results.csv"):
    """
    Perform manual NER analysis to identify specific entities in articles dataset.
    
    Args:
        articles_path (str): Path to the articles CSV file
        output_path (str): Path to save the results CSV file
    
    Returns:
        pandas.DataFrame: Results dataframe with entity mentions
    """
    
    print("üîç STARTING MANUAL NER ANALYSIS")
    print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # Define entities with all their potential variants for better matching
    entity_data = [
        {
            'Normalized_Entity': '–ê–≤—Ç–æ–¥–æ—Ä',
            'Entity_Type': 'Organization',
            'Variants': [
                'Avtodorozhnaya Stroitelnaya Korporatsiya, LLC', 'Avtodorozhnaya Stroitelnaya Korporatsiya',
                '–ê–≤—Ç–æ–¥–æ—Ä', '–ê–≤—Ç–æ–¥–æ—Ä–∞', '–ê–≤—Ç–æ–¥–æ—Ä—É', '–∫–æ–º–ø–∞–Ω–∏—è –ê–≤—Ç–æ–¥–æ—Ä', '–ê–û –ê–≤—Ç–æ–¥–æ—Ä',
                '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è –ê–≤—Ç–æ–¥–æ—Ä', '–ì–ö –ê–≤—Ç–æ–¥–æ—Ä'
            ]
        },
        {
            'Normalized_Entity': '–ø—Ä–∏–Ω—Ü –ú—É—Ö–∞–º–º–µ–¥ –±–µ–Ω –°–∞–ª—å–º–∞–Ω –ê–ª—å –°–∞—É–¥ (MBS)',
            'Entity_Type': 'Person',
            'Variants': [
                'Mohammed bin Salman (MBS)', 'Mohammed bin Salman', 'MBS',
                '–ø—Ä–∏–Ω—Ü –ú—É—Ö–∞–º–º–µ–¥ –±–µ–Ω –°–∞–ª—å–º–∞–Ω', '–ú—É—Ö–∞–º–º–µ–¥ –±–µ–Ω –°–∞–ª—å–º–∞–Ω', '–Ω–∞—Å–ª–µ–¥–Ω—ã–π –ø—Ä–∏–Ω—Ü –°–∞—É–¥–æ–≤—Å–∫–æ–π –ê—Ä–∞–≤–∏–∏',
                '–ø—Ä–∏–Ω—Ü–∞ –ú—É—Ö–∞–º–º–µ–¥–∞ –±–µ–Ω –°–∞–ª—å–º–∞–Ω–∞', '–ø—Ä–∏–Ω—Ü—É –ú—É—Ö–∞–º–º–µ–¥—É –±–µ–Ω –°–∞–ª—å–º–∞–Ω—É',
                '–Ω–∞—Å–ª–µ–¥–Ω—ã–π –ø—Ä–∏–Ω—Ü', '–∫—Ä–æ–Ω–ø—Ä–∏–Ω—Ü –°–∞—É–¥–æ–≤—Å–∫–æ–π –ê—Ä–∞–≤–∏–∏'
            ]
        },
        {
            'Normalized_Entity': '–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ß–æ–±–æ—Ç–æ–≤',
            'Entity_Type': 'Person',
            'Variants': [
                'Alexander Chobotov', '–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ß–æ–±–æ—Ç–æ–≤', '–ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ß–æ–±–æ—Ç–æ–≤–∞',
                '–ê–ª–µ–∫—Å–∞–Ω–¥—Ä—É –ß–æ–±–æ—Ç–æ–≤—É', '–ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–º –ß–æ–±–æ—Ç–æ–≤—ã–º', '–æ–± –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–µ –ß–æ–±–æ—Ç–æ–≤–µ',
                '–ê. –ß–æ–±–æ—Ç–æ–≤', '–ß–æ–±–æ—Ç–æ–≤'
            ]
        },
        {
            'Normalized_Entity': '–†–∏–∫ –ì–µ—Ä—Å–æ–Ω',
            'Entity_Type': 'Person',
            'Variants': [
                'Rick Gerson', '–†–∏–∫ –ì–µ—Ä—Å–æ–Ω', '–†–∏–∫–∞ –ì–µ—Ä—Å–æ–Ω–∞', '–†–∏–∫—É –ì–µ—Ä—Å–æ–Ω—É',
                '–†–∏–∫–æ–º –ì–µ—Ä—Å–æ–Ω–æ–º', '–æ –†–∏–∫–µ –ì–µ—Ä—Å–æ–Ω–µ', '–†. –ì–µ—Ä—Å–æ–Ω', '–ì–µ—Ä—Å–æ–Ω'
            ]
        },
        {
            'Normalized_Entity': '–†—É—Å—ç–Ω–µ—Ä–≥–æ—Å–±—ã—Ç',
            'Entity_Type': 'Organization',
            'Variants': [
                'Rusenergosbyt', '–†—É—Å—ç–Ω–µ—Ä–≥–æ—Å–±—ã—Ç', '–†—É—Å—ç–Ω–µ—Ä–≥–æ—Å–±—ã—Ç–∞', '–†—É—Å—ç–Ω–µ—Ä–≥–æ—Å–±—ã—Ç—É',
                '–∫–æ–º–ø–∞–Ω–∏—è –†—É—Å—ç–Ω–µ—Ä–≥–æ—Å–±—ã—Ç', '–û–û–û –†—É—Å—ç–Ω–µ—Ä–≥–æ—Å–±—ã—Ç', '–≥—Ä—É–ø–ø–∞ –†—É—Å—ç–Ω–µ—Ä–≥–æ—Å–±—ã—Ç'
            ]
        },
        {
            'Normalized_Entity': '–ì–∞–∑–ø—Ä–æ–º –ù–µ—Ñ—Ç—å –í–æ—Å—Ç–æ–∫',
            'Entity_Type': 'Organization',
            'Variants': [
                'Gazpromneft-Vostok JV', '–ì–∞–∑–ø—Ä–æ–º –ù–µ—Ñ—Ç—å –í–æ—Å—Ç–æ–∫', '–ì–∞–∑–ø—Ä–æ–º–∞ –ù–µ—Ñ—Ç—å –í–æ—Å—Ç–æ–∫',
                '–ì–∞–∑–ø—Ä–æ–º—É –ù–µ—Ñ—Ç—å –í–æ—Å—Ç–æ–∫', '–∫–æ–º–ø–∞–Ω–∏—è –ì–∞–∑–ø—Ä–æ–º –ù–µ—Ñ—Ç—å –í–æ—Å—Ç–æ–∫',
                '–°–ü –ì–∞–∑–ø—Ä–æ–º –ù–µ—Ñ—Ç—å –í–æ—Å—Ç–æ–∫', '—Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ –ì–∞–∑–ø—Ä–æ–º –ù–µ—Ñ—Ç—å –í–æ—Å—Ç–æ–∫'
            ]
        },
        {
            'Normalized_Entity': 'NIIF',
            'Entity_Type': 'Organization',
            'Variants': [
                'National Investment and Infrastructure Fund (NIIF)', 'National Investment and Infrastructure Fund',
                'NIIF', '–ù–ò–§', '–ù–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π –∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π —Ñ–æ–Ω–¥',
                '–ù–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π —Ñ–æ–Ω–¥', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π —Ñ–æ–Ω–¥ NIIF'
            ]
        },
        {
            'Normalized_Entity': '–ï–≤—Ä–∞–∑–∏–π—Å–∫–∏–π –±–∞–Ω–∫ —Ä–∞–∑–≤–∏—Ç–∏—è (EDB)',
            'Entity_Type': 'Organization',
            'Variants': [
                'Eurasian Development Bank (EDB)', 'Eurasian Development Bank', 'EDB', '–ï–ë–†',
                '–ï–≤—Ä–∞–∑–∏–π—Å–∫–∏–π –±–∞–Ω–∫ —Ä–∞–∑–≤–∏—Ç–∏—è', '–ï–≤—Ä–∞–∑–∏–π—Å–∫–æ–≥–æ –±–∞–Ω–∫–∞ —Ä–∞–∑–≤–∏—Ç–∏—è', '–±–∞–Ω–∫ –ï–ë–†'
            ]
        },
        {
            'Normalized_Entity': '–ó–∞–ø–°–∏–±–ù–µ—Ñ—Ç–µ—Ö–∏–º',
            'Entity_Type': 'Organization',
            'Variants': [
                'ZapSibNeftekhim', '–ó–∞–ø–°–∏–±–ù–µ—Ñ—Ç–µ—Ö–∏–º', '–ó–∞–ø–°–∏–±–ù–µ—Ñ—Ç–µ—Ö–∏–º–∞', '–ó–∞–ø–°–∏–±–ù–µ—Ñ—Ç–µ—Ö–∏–º—É',
                '–∫–æ–º–ø–∞–Ω–∏—è –ó–∞–ø–°–∏–±–ù–µ—Ñ—Ç–µ—Ö–∏–º', '–∑–∞–≤–æ–¥ –ó–∞–ø–°–∏–±–ù–µ—Ñ—Ç–µ—Ö–∏–º', '–∫–æ–º–ø–ª–µ–∫—Å –ó–∞–ø–°–∏–±–ù–µ—Ñ—Ç–µ—Ö–∏–º'
            ]
        },
        {
            'Normalized_Entity': 'T√ºrkiye Wealth Fund (TWF)',
            'Entity_Type': 'Organization',
            'Variants': [
                'T√ºrkiye Wealth Fund (TWF)', 'T√ºrkiye Wealth Fund', 'TWF',
                '–¢—É—Ä–µ—Ü–∫–∏–π —Ñ–æ–Ω–¥ –±–ª–∞–≥–æ—Å–æ—Å—Ç–æ—è–Ω–∏—è', '–§–æ–Ω–¥ –±–ª–∞–≥–æ—Å–æ—Å—Ç–æ—è–Ω–∏—è –¢—É—Ä—Ü–∏–∏',
                '—Å—É–≤–µ—Ä–µ–Ω–Ω—ã–π —Ñ–æ–Ω–¥ –¢—É—Ä—Ü–∏–∏', '—Ç—É—Ä–µ—Ü–∫–∏–π —Å—É–≤–µ—Ä–µ–Ω–Ω—ã–π —Ñ–æ–Ω–¥'
            ]
        },
        {
            'Normalized_Entity': 'Saudi Basic Industries Corporation (SABIC)',
            'Entity_Type': 'Organization',
            'Variants': [
                'Saudi Basic Industries Corporation (SABIC)', 'Saudi Basic Industries Corporation',
                'SABIC', '–°–ê–ë–ò–ö', '–°–∞—É–¥–æ–≤—Å–∫–∞—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è –±–∞–∑–æ–≤—ã—Ö –æ—Ç—Ä–∞—Å–ª–µ–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏',
                '–∫–æ–º–ø–∞–Ω–∏—è –°–ê–ë–ò–ö', '–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è –°–ê–ë–ò–ö'
            ]
        },
        {
            'Normalized_Entity': '–®–æ–Ω –ì–ª–æ–±–µ–∫',
            'Entity_Type': 'Person',
            'Variants': [
                'Sean Glodek', '–®–æ–Ω –ì–ª–æ–±–µ–∫', '–®–æ–Ω–∞ –ì–ª–æ–±–µ–∫–∞', '–®–æ–Ω—É –ì–ª–æ–±–µ–∫—É',
                '–®–æ–Ω–æ–º –ì–ª–æ–±–µ–∫–æ–º', '–®. –ì–ª–æ–±–µ–∫', '–ì–ª–æ–±–µ–∫'
            ]
        },
        {
            'Normalized_Entity': '–†–æ—Å—Å–∏–π—Å–∫–æ-—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π —Ñ–æ–Ω–¥',
            'Entity_Type': 'Organization',
            'Variants': [
                'Russia-France Investment Fund (RFIF)', 'Russia-France Investment Fund',
                '–†–æ—Å—Å–∏–π—Å–∫–æ-—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π —Ñ–æ–Ω–¥', '–†–§–ò–§',
                '—Ä–æ—Å—Å–∏–π—Å–∫–æ-—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π —Ñ–æ–Ω–¥', '—Ñ–æ–Ω–¥ –†–§–ò–§'
            ]
        },
        {
            'Normalized_Entity': '–¢–∞–¥–∞—à–∏ –ú–∞—ç–¥–∞',
            'Entity_Type': 'Person',
            'Variants': [
                'Tadashi Maeda', '–¢–∞–¥–∞—à–∏ –ú–∞—ç–¥–∞', '–¢–∞–¥–∞—à–∏ –ú–∞—ç–¥—ã', '–¢–∞–¥–∞—à–∏ –ú–∞—ç–¥–µ',
                '–¢. –ú–∞—ç–¥–∞', '–ú–∞—ç–¥–∞'
            ]
        },
        {
            'Normalized_Entity': '–ø—Ä–∏–Ω—Ü –ë–∞–Ω–¥–∞—Ä –±–µ–Ω –°—É–ª—Ç–∞–Ω',
            'Entity_Type': 'Person',
            'Variants': [
                'Prince Bandar bin Sultan', '–ø—Ä–∏–Ω—Ü –ë–∞–Ω–¥–∞—Ä –±–µ–Ω –°—É–ª—Ç–∞–Ω', '–ø—Ä–∏–Ω—Ü–∞ –ë–∞–Ω–¥–∞—Ä–∞ –±–µ–Ω –°—É–ª—Ç–∞–Ω–∞',
                '–ø—Ä–∏–Ω—Ü—É –ë–∞–Ω–¥–∞—Ä—É –±–µ–Ω –°—É–ª—Ç–∞–Ω', '–ë–∞–Ω–¥–∞—Ä –±–µ–Ω –°—É–ª—Ç–∞–Ω', '–ø—Ä–∏–Ω—Ü –ë–∞–Ω–¥–∞—Ä'
            ]
        },
        {
            'Normalized_Entity': '–ê–Ω–≤–∞—Ä –ò–±—Ä–∞–≥–∏–º',
            'Entity_Type': 'Person',
            'Variants': [
                'Anwar Ibrahim', '–ê–Ω–≤–∞—Ä –ò–±—Ä–∞–≥–∏–º', '–ê–Ω–≤–∞—Ä–∞ –ò–±—Ä–∞–≥–∏–º–∞', '–ê–Ω–≤–∞—Ä—É –ò–±—Ä–∞–≥–∏–º—É',
                '–ê. –ò–±—Ä–∞–≥–∏–º', '–ò–±—Ä–∞–≥–∏–º'
            ]
        },
        {
            'Normalized_Entity': '–í—Å–µ–º–∏—Ä–Ω—ã–π —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π —Ñ–æ—Ä—É–º',
            'Entity_Type': 'Organization',
            'Variants': [
                'World Economic Forum', '–í—Å–µ–º–∏—Ä–Ω—ã–π —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π —Ñ–æ—Ä—É–º', '–í–≠–§',
                '–î–∞–≤–æ—Å—Å–∫–∏–π —Ñ–æ—Ä—É–º', '—Ñ–æ—Ä—É–º –≤ –î–∞–≤–æ—Å–µ', '—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π —Ñ–æ—Ä—É–º –≤ –î–∞–≤–æ—Å–µ'
            ]
        },
        {
            'Normalized_Entity': '–ß–æ–Ω –°—É–∫ –ß–æ–π',
            'Entity_Type': 'Person',
            'Variants': [
                'Chong-Suk Choi', '–ß–æ–Ω –°—É–∫ –ß–æ–π', '–ß–æ–Ω –°—É–∫–∞ –ß–æ—è', '–ß–æ–Ω –°—É–∫—É –ß–æ—é',
                '–ß–æ–π', '–ß.–°. –ß–æ–π'
            ]
        },
        {
            'Normalized_Entity': '–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è –ø–æ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è–º –∫–∞–ø–∏—Ç–∞–ª–∞ (SCIC)',
            'Entity_Type': 'Organization',
            'Variants': [
                'State Capital Investment Corporation (SCIC)', 'State Capital Investment Corporation',
                'SCIC', '–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è –ø–æ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è–º –∫–∞–ø–∏—Ç–∞–ª–∞',
                '–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è SCIC', '–≤—å–µ—Ç–Ω–∞–º—Å–∫–∞—è SCIC'
            ]
        },
        {
            'Normalized_Entity': 'JPMorgan Chase & Co.',
            'Entity_Type': 'Organization',
            'Variants': [
                'JPMorgan Chase & Co.', 'JPMorgan Chase', 'JPMorgan',
                '–î–∂–µ–π –ü–∏ –ú–æ—Ä–≥–∞–Ω –ß–µ–π–∑', '–î–∂–µ–π–ü–∏–ú–æ—Ä–≥–∞–Ω', '–±–∞–Ω–∫ JPMorgan'
            ]
        },
        {
            'Normalized_Entity': 'BlackRock',
            'Entity_Type': 'Organization',
            'Variants': [
                'BlackRock', '–ë–ª—ç–∫–†–æ–∫', '–∫–æ–º–ø–∞–Ω–∏—è BlackRock', '—Ñ–æ–Ω–¥ BlackRock'
            ]
        },
        {
            'Normalized_Entity': 'KIA',
            'Entity_Type': 'Organization',
            'Variants': [
                'Korea Investment Corporation', 'KIA', '–ö–ò–ê',
                '–ö–æ—Ä–µ–π—Å–∫–∞—è –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–∞—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è', '–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è KIA'
            ]
        },
        {
            'Normalized_Entity': 'Mitsui & Co.',
            'Entity_Type': 'Organization',
            'Variants': [
                'Mitsui & Co.', 'Mitsui', '–ú–∏—Ü—É–∏ —ç–Ω–¥ –ö–æ', '–∫–æ–º–ø–∞–Ω–∏—è –ú–∏—Ü—É–∏',
                '–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è –ú–∏—Ü—É–∏', '–≥—Ä—É–ø–ø–∞ –ú–∏—Ü—É–∏'
            ]
        },
        {
            'Normalized_Entity': '–§–æ–Ω–¥ –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –±–ª–∞–≥–æ—Å–æ—Å—Ç–æ—è–Ω–∏—è',
            'Entity_Type': 'Organization',
            'Variants': [
                'National Wealth Fund', '–§–æ–Ω–¥ –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –±–ª–∞–≥–æ—Å–æ—Å—Ç–æ—è–Ω–∏—è', '–§–ù–ë',
                '—Ä–æ—Å—Å–∏–π—Å–∫–∏–π –§–ù–ë', '–Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–æ–Ω–¥ –±–ª–∞–≥–æ—Å–æ—Å—Ç–æ—è–Ω–∏—è'
            ]
        }
        # Adding more entities would follow the same pattern...
    ]
    
    # Read the articles dataset
    try:
        if not os.path.exists(articles_path):
            raise FileNotFoundError(f"Articles file not found at: {articles_path}")
        
        df_articles = pd.read_csv(articles_path)
        print(f"‚úì Successfully loaded articles dataset: {articles_path}")
        print(f"‚úì Total articles: {len(df_articles):,}")
        print(f"‚úì Columns: {list(df_articles.columns)}")
        
    except Exception as e:
        print(f"‚ùå Error loading articles dataset: {e}")
        return None
    
    # Prepare results list
    results = []
    
    # Context window size (characters before and after the match)
    context_window = 50
    
    # Precompile regex patterns for all variants (case-insensitive, word boundaries)
    for entity in entity_data:
        entity['Compiled_Patterns'] = []
        for variant in entity['Variants']:
            # Use word boundaries for better matching, escape special regex characters
            pattern = r'\b' + re.escape(variant) + r'\b'
            try:
                entity['Compiled_Patterns'].append(re.compile(pattern, re.IGNORECASE))
            except re.error:
                # Fallback for problematic patterns
                entity['Compiled_Patterns'].append(re.compile(re.escape(variant), re.IGNORECASE))
    
    print(f"\nüîç Analyzing {len(entity_data)} entities across {len(df_articles)} articles...")
    
    # Process each article
    for idx, row in df_articles.iterrows():
        if idx % 1000 == 0:
            print(f"   Processing article {idx+1:,} of {len(df_articles):,}")
        
        article_id = row.get('article_id', f'article_{idx}')
        date = row.get('date', '')
        source = row.get('source', '')
        
        # Combine title and body for text analysis
        title_text = str(row.get('title', ''))
        body_text = str(row.get('body', ''))
        full_text = f"{title_text} {body_text}"
        
        # Skip if no meaningful text
        if len(full_text.strip()) < 10:
            continue
        
        text_len = len(full_text)
        
        # Search for each entity and its variants
        for entity in entity_data:
            normalized_entity = entity['Normalized_Entity']
            entity_type = entity['Entity_Type']
            
            entity_mentions = []
            
            # Search all variant patterns
            for pattern in entity['Compiled_Patterns']:
                for match in pattern.finditer(full_text):
                    start, end = match.start(), match.end()
                    matched_text = match.group()
                    
                    # Extract context around the match
                    context_start = max(0, start - context_window)
                    context_end = min(text_len, end + context_window)
                    context_text = full_text[context_start:context_end].strip()
                    
                    # Clean up context text
                    context_text = re.sub(r'\s+', ' ', context_text)  # Remove extra whitespace
                    context_text = context_text.replace('\n', ' ').replace('\t', ' ')
                    
                    entity_mentions.append({
                        'matched_text': matched_text,
                        'context': context_text,
                        'position': (start, end)
                    })
            
            # Add results for this entity in this article
            if entity_mentions:
                # Remove duplicate mentions at the same position
                unique_mentions = []
                seen_positions = set()
                for mention in entity_mentions:
                    if mention['position'] not in seen_positions:
                        unique_mentions.append(mention)
                        seen_positions.add(mention['position'])
                
                # Create one entry per unique mention
                for mention in unique_mentions:
                    results.append({
                        'Article_ID': article_id,
                        'Date': date,
                        'Source': source,
                        'Entity': normalized_entity,
                        'Entity_Type': entity_type,
                        'Occurrences': 1,
                        'Context_Text': mention['context']
                    })
    
    # Create DataFrame from results
    if results:
        results_df = pd.DataFrame(results)
        
        # Sort by article ID and entity for better organization
        results_df = results_df.sort_values(['Article_ID', 'Entity', 'Date'])
        
        # Save results to CSV
        results_df.to_csv(output_path, index=False, encoding='utf-8')
        
        # Print summary statistics
        print(f"\n{'='*60}")
        print(f"MANUAL NER ANALYSIS RESULTS")
        print(f"{'='*60}")
        print(f"‚úì Total entity mentions found: {len(results_df):,}")
        print(f"‚úì Unique entities found: {results_df['Entity'].nunique()}")
        print(f"‚úì Articles with mentions: {results_df['Article_ID'].nunique()}")
        print(f"‚úì Results saved to: {output_path}")
        
        # Show top entities by frequency
        print(f"\nüìä TOP 10 MOST MENTIONED ENTITIES:")
        top_entities = results_df['Entity'].value_counts().head(10)
        for i, (entity, count) in enumerate(top_entities.items(), 1):
            print(f"{i:>2}. {entity}: {count} mentions")
        
        # Show entity type distribution
        print(f"\nüìä ENTITY TYPE DISTRIBUTION:")
        type_dist = results_df['Entity_Type'].value_counts()
        for entity_type, count in type_dist.items():
            print(f"   {entity_type}: {count} mentions")
        
        return results_df
        
    else:
        print(f"\n‚ö†Ô∏è  No entity mentions found in the articles dataset")
        # Create empty DataFrame with correct structure
        empty_df = pd.DataFrame(columns=['Article_ID', 'Date', 'Source', 'Entity', 'Entity_Type', 'Occurrences', 'Context_Text'])
        empty_df.to_csv(output_path, index=False, encoding='utf-8')
        print(f"‚úì Empty results file created: {output_path}")
        return empty_df


def analyze_ner_results(results_path="manual_ner_analysis_results.csv"):
    """
    Analyze the NER results and provide detailed statistics.
    """
    try:
        df = pd.read_csv(results_path)
        
        print(f"\n{'='*60}")
        print(f"DETAILED NER ANALYSIS REPORT")
        print(f"{'='*60}")
        
        # Overall statistics
        print(f"üìà OVERALL STATISTICS:")
        print(f"   Total mentions: {len(df):,}")
        print(f"   Unique entities: {df['Entity'].nunique()}")
        print(f"   Unique articles: {df['Article_ID'].nunique()}")
        print(f"   Unique sources: {df['Source'].nunique()}")
        
        # Entity frequency analysis
        print(f"\nüìä ENTITY FREQUENCY ANALYSIS:")
        entity_freq = df['Entity'].value_counts()
        print(f"   Most mentioned entity: {entity_freq.index[0]} ({entity_freq.iloc[0]} mentions)")
        print(f"   Average mentions per entity: {entity_freq.mean():.1f}")
        print(f"   Median mentions per entity: {entity_freq.median():.1f}")
        
        # Source analysis
        if 'Source' in df.columns and df['Source'].notna().any():
            print(f"\nüì∞ SOURCE ANALYSIS:")
            source_freq = df['Source'].value_counts().head(5)
            for source, count in source_freq.items():
                print(f"   {source}: {count} mentions")
        
        # Monthly analysis if dates are available
        if 'Date' in df.columns and df['Date'].notna().any():
            print(f"\nüìÖ TEMPORAL ANALYSIS:")
            try:
                df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
                df['Month'] = df['Date'].dt.to_period('M')
                monthly_mentions = df['Month'].value_counts().sort_index().tail(6)
                for month, count in monthly_mentions.items():
                    print(f"   {month}: {count} mentions")
            except:
                print("   Could not analyze temporal patterns")
        
        return df
        
    except FileNotFoundError:
        print(f"‚ùå Results file not found: {results_path}")
        return None


# Main execution
if __name__ == "__main__":
    print("üöÄ MANUAL NER ANALYSIS FOR SPECIFIED ENTITIES")
    print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Run the manual NER analysis
    results_df = manual_ner_analysis(
        articles_path="Articles/cleaned_articles_combined.csv",
        output_path="manual_ner_analysis_results.csv"
    )
    
    if results_df is not None and len(results_df) > 0:
        # Analyze the results
        analyze_ner_results("manual_ner_analysis_results.csv")
        
        print(f"\n‚úÖ ANALYSIS COMPLETED SUCCESSFULLY!")
        print(f"‚úì Results file: 'manual_ner_analysis_results.csv'")
        print(f"‚úì Dataset structure: Article_ID, Date, Source, Entity, Entity_Type, Occurrences, Context_Text")
    else:
        print(f"\n‚ö†Ô∏è  Analysis completed but no entity mentions were found")
    
    print(f"\nFinished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
