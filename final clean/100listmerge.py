import pandas as pd
from datetime import datetime

def process_entity_datasets():
    """
    Process NER entity datasets to:
    1. Merge similar entities 
    2. Fix spelling errors
    3. Filter to keep only specified entities
    4. Save changes to original files
    """
    
    print("üîß PROCESSING ENTITY DATASETS")
    print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # Define the dataset filenames
    files = [
        'ner_entity_dataset_TOP_100.csv',
        'ner_entity_dataset_TOP_100_UNIQUE.csv'
    ]
    
    # Define renaming map for merging and fixing spelling errors
    rename_map = {
        # MERGE entities
        '–ï–∫–∞—Ç–µ—Ä–∏–Ω—ã –¢–∏—Ö–æ–Ω–æ–≤–æ–π': '–ï–∫–∞—Ç–µ—Ä–∏–Ω–∞ –¢–∏—Ö–æ–Ω–æ–≤–∞',
        '–¢–∏—Ö–æ–Ω–æ–≤–∞': '–ï–∫–∞—Ç–µ—Ä–∏–Ω–∞ –¢–∏—Ö–æ–Ω–æ–≤–∞',
        
        # FIX SPELLING
        '–≠—Ä–∏–∫–æ–º –ü—Ä–∏–Ω—Å–æ–º': '–≠—Ä–∏–∫ –ü—Ä–∏–Ω—Å',
        '–°—É–º–º—ã': '–°—É–º–º–∞',
        '–°—Ç—Ä–æ—Å—Å-–ö–∞–Ω': '–î–æ–º–∏–Ω–∏–∫ –°—Ç—Ä–æ—Å—Å-–ö–∞–Ω',
        '–†–æ—Å—Å–µ—Ç—è—Ö': '¬´–†–æ—Å—Å–µ—Ç–∏¬ª',
        '–†–æ—Å–∞—Ç–æ–º–æ–º': '–†–æ—Å–∞—Ç–æ–º',
        '–ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –î–∂–∞–ø–∞—Ä–∏–¥–∑–µ': '–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –î–∂–∞–ø–∞—Ä–∏–¥–∑–µ',
        '–û–ê–û –î–µ—Ç—Å–∫–∏–π –º–∏—Ä': '–î–µ—Ç—Å–∫–∏–π –º–∏—Ä',
        '–î–µ—Ç—Å–∫–∏–º –º–∏—Ä–æ–º': '–î–µ—Ç—Å–∫–∏–π –º–∏—Ä',
        '–ï–≤—Ä–æ—Ö–∏–º–∞': '–ï–≤—Ä–æ–•–∏–º',
        '–ê–ª—Ä–æ—Å–µ': '–ê–ö –ê–õ–†–û–°–ê',
        '–î–º–∏—Ç—Ä–∏—è –°–µ—Ä–≥–µ–µ–≤–∞': '–î–º–∏—Ç—Ä–∏–π –°–µ—Ä–≥–µ–µ–≤',
        'Mubadala Development': 'Mubadala Investment Company',
        '–ú–æ–∏—Å–µ–µ–≤': '–ê–ª–µ–∫—Å–µ–π –ú–æ–∏—Å–µ–µ–≤',
        'DP World Russia': 'DP World',
        '–ê–Ω–¥—Ä–µ–µ–º –ë–µ–ª–æ—É—Å–æ–≤—ã–º': '–ê–Ω–¥—Ä–µ–π –ë–µ–ª–æ—É—Å–æ–≤'
    }
    
    # Define entities to keep after filtering
    keep_entities = {
        'AGC Equity Partners',
        'Almaz Capital Partners',
        'Altera Capital',
        'Apollo Global Management',
        'Arc International',
        'CDC International Capital',
        'Cassa Depositi e Prestiti SpA',
        'Dr Reddys',
        'Dubai Ports',
        'Franklin Templeton',
        'Fraport AG',
        'Gland Pharma',
        'Hetero Biopharma',
        'JBIC IG Partners',
        'RTP Global',
        'Russia Forest Products',
        'Serum Institute of India',
        'Stelis Biopharma',
        'Tata Power',
        'Tesla Motors',
        'UFG Private Equity',
        'Virchow Biotech',
        'Warburg Pincus',
        '–ê–§–ì –ù–∞—Ü–∏–æ–Ω–∞–ª—å',
        '–ê–≠–° –ê–∫–∫—É—é',
        '–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ê—Ñ–∞–Ω–∞—Å—å–µ–≤',
        '–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –î–∂–∞–ø–∞—Ä–∏–¥–∑–µ',  # Updated name after renaming
        '–ê–º–∏–Ω –ù–∞—Å—Å–µ—Ä',
        '–ê–Ω–∞—Ç–æ–ª–∏–π –ë—Ä–∞–≤–µ—Ä–º–∞–Ω',
        '–ë–∞–¥–µ—Ä –ú–æ—Ö–∞–º–º–∞–¥ –ê–ª—å-–°–∞–∞–¥',
        '–í–ª–∞–¥–∏–º–∏—Ä –†–æ–∂–∞–Ω–∫–æ–≤—Å–∫–∏–π',
        '–í–æ–ª–æ–∂',
        '–í–æ–ª—Ç–∞–π—Ä-–ü—Ä–æ–º',
        '–í—ç–Ω—å –¶–∑—è–±–∞–æ',
        '–ì–µ—Ä–æ—Ñ–∞—Ä–º',
        '–ì–æ–Ω–∫–æ–Ω–≥—Å–∫–æ–π –±–∏—Ä–∂–µ',
        '–î–µ–ª—å–ø–∞–ª—å',
        '–î–µ—Ç—Å–∫–∞—è –ì–∞–ª–µ—Ä–µ—è –Ø–∫–∏–º–∞–Ω–∫–∞',
        '–î–∏–∫—Å–∏',
        '–î–º–∏—Ç—Ä–∏–π –°–µ—Ä–≥–µ–µ–≤',  # Updated name after renaming
        '–ï–≤–≥–µ–Ω–∏–π –Æ—Ä—á–µ–Ω–∫–æ',
        '–ï–≤—Ä–ê–∑–≠–°',
        '–ï–≤—Ä–æ–•–∏–º',  # Updated name after renaming
        '–ó–ê–û –¢–µ—Ä–º–∏–Ω–∞–ª –í–ª–∞–¥–∏–≤–æ—Å—Ç–æ–∫',
        '–ó–∞–ø–°–∏–±-2',
        '–ó–æ–æ–∑–∞–≤—Ä',
        '–ò–≤–ª–µ–≤',
        '–ò—Ä–∏–Ω–∞ –ü–∞—Ä—Ñ–µ–Ω—Ç—å–µ–≤–∞',
        '–ö–ò–¢ –§–∏–Ω–∞–Ω—Å',
        '–ö–∞—Ä–æ –§–∏–ª—å–º',
        '–ö–∏—Ä–∞ –ö–∏—Ä—é—Ö–∏–Ω–∞',
        '–õ–µ–æ–Ω –ë–ª—ç–∫',
        '–õ–µ–æ–Ω–∏–¥ –ë–æ–≥—É—Å–ª–∞–≤—Å–∫–∏–π',
        '–õ—é –¶–∑–∏–≤—ç–π',
        '–ú–∞–∫—Å–∏–º –ü–µ—Ä–µ–ª—å–º–∞–Ω',
        '–ú–∞—É—Ä–∏—Ü–∏–æ –¢–∞–º–∞–Ω—å–∏–Ω–∏',
        '–ú–∏—Ö–∞–∏–ª –ê–ª–µ–∫—Å–µ–µ–≤',
        '–ú–∏—Ö–∞–∏–ª –ö–æ–≤–∞–ª—å—á—É–∫',
        '–ù–µ—Ñ—Ç–µ—Ç—Ä–∞–Ω—Å—Å–µ—Ä–≤–∏—Å',
        '–ù–∏—Ö–∞—Ç –ó–µ–π–±–µ–∫—á–∏',
        '–ù–æ–≤–∞–ø–æ—Ä—Ç',
        '–ù–æ–≤—ã–π –±–∏–∑–Ω–µ—Å',
        '–û–ª–µ–≥ –¢—Ä—É—Ç–Ω–µ–≤',
        '–û–ª–∏–º–ø–∏–∫ –ü–∞—Ä–∫',
        '–ü–∞–≤–µ–ª –¢–µ–ø–ª—É—Ö–∏–Ω',
        '–†–¢-–ò–Ω–≤–µ—Å—Ç',
        '–†–µ–º–¥–µ—Å–∏–≤–∏—Ä–∞',
        '–°—Ç–∏–≤–µ–Ω –®–≤–∞—Ä—Ü–º–∞–Ω',
        '–¢–∞–≥–∏—Ä –°–∏—Ç–¥–µ–∫–æ–≤',
        '–¢–µ–¥—Ä–æ—Å –ì–µ–±—Ä–µ–π–µ—Å—É—Å',
        '–¢–∏–Ω–∞ –ö–∞–Ω–¥–µ–ª–∞–∫–∏',
        '–¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å –¢–µ–ª–µ–∫–æ–º',
        '–£—Ä–∞–ª—Ö–∏–º',
        '–ß–∂–∞–Ω –ì–∞–æ–ª–∏',
        '–≠–ª–µ–∫—Ç—Ä–æ—â–∏—Ç –°–∞–º–∞—Ä–∞',
        '–≠–Ω—Ä–∏–∫–æ –õ–µ—Ç—Ç—ã',
        # Additional entities that might result from renaming
        '–ï–∫–∞—Ç–µ—Ä–∏–Ω–∞ –¢–∏—Ö–æ–Ω–æ–≤–∞',  # Result of merging
        '–≠—Ä–∏–∫ –ü—Ä–∏–Ω—Å',  # Result of spelling fix
        '–°—É–º–º–∞',  # Result of spelling fix
        '–î–æ–º–∏–Ω–∏–∫ –°—Ç—Ä–æ—Å—Å-–ö–∞–Ω',  # Result of spelling fix
        '¬´–†–æ—Å—Å–µ—Ç–∏¬ª',  # Result of spelling fix
        '–†–æ—Å–∞—Ç–æ–º',  # Result of spelling fix
        '–î–µ—Ç—Å–∫–∏–π –º–∏—Ä',  # Result of merging
        '–ê–ö –ê–õ–†–û–°–ê',  # Result of spelling fix
        'Mubadala Investment Company',  # Result of spelling fix
        '–ê–ª–µ–∫—Å–µ–π –ú–æ–∏—Å–µ–µ–≤',  # Result of spelling fix
        'DP World',  # Result of spelling fix
        '–ê–Ω–¥—Ä–µ–π –ë–µ–ª–æ—É—Å–æ–≤'  # Result of spelling fix
    }
    
    def process_file(filename):
        """Process a single CSV file"""
        try:
            # Load the dataset
            df = pd.read_csv(filename)
            print(f"‚úì Loaded {filename}: {len(df)} rows")
            
            if 'Entity' not in df.columns:
                print(f"‚ùå Error: 'Entity' column not found in {filename}")
                return None, None
            
            original_entities = df['Entity'].nunique()
            
            # Step 1: Apply renaming (merging and spelling fixes)
            df['Entity'] = df['Entity'].replace(rename_map)
            
            # Count entities after renaming
            renamed_entities = df['Entity'].nunique()
            
            # Step 2: Filter to keep only specified entities
            df_filtered = df[df['Entity'].isin(keep_entities)].copy()
            
            # Step 3: Save the fully processed dataset back to original file
            df.to_csv(filename, index=False)
            print(f"‚úì Saved updated {filename}")
            
            # Step 4: Create a filtered version
            filtered_filename = filename.replace('.csv', '_filtered.csv')
            df_filtered.to_csv(filtered_filename, index=False)
            print(f"‚úì Created filtered version: {filtered_filename}")
            
            # Print statistics
            print(f"   - Original unique entities: {original_entities}")
            print(f"   - After renaming: {renamed_entities}")
            print(f"   - After filtering: {df_filtered['Entity'].nunique()}")
            print(f"   - Rows kept after filtering: {len(df_filtered)} of {len(df)}")
            
            return df, df_filtered
            
        except FileNotFoundError:
            print(f"‚ùå Error: File '{filename}' not found.")
            return None, None
        except Exception as e:
            print(f"‚ùå Error processing {filename}: {e}")
            return None, None
    
    # Process both files
    results = {}
    
    for filename in files:
        print(f"\n{'='*40}")
        print(f"PROCESSING: {filename}")
        print(f"{'='*40}")
        
        original_df, filtered_df = process_file(filename)
        
        if original_df is not None:
            results[filename] = {
                'original': original_df,
                'filtered': filtered_df,
                'success': True
            }
        else:
            results[filename] = {'success': False}
    
    # Print final summary
    print(f"\n{'='*60}")
    print(f"PROCESSING SUMMARY")
    print(f"{'='*60}")
    
    successful_files = [f for f, r in results.items() if r['success']]
    failed_files = [f for f, r in results.items() if not r['success']]
    
    print(f"‚úÖ Successfully processed: {len(successful_files)} files")
    for f in successful_files:
        print(f"   - {f}")
    
    if failed_files:
        print(f"‚ùå Failed to process: {len(failed_files)} files")
        for f in failed_files:
            print(f"   - {f}")
    
    print(f"\nüìã RENAMING RULES APPLIED:")
    for old_name, new_name in rename_map.items():
        print(f"   '{old_name}' ‚Üí '{new_name}'")
    
    print(f"\nüîç FILTERING APPLIED:")
    print(f"   Kept only {len(keep_entities)} specified entities")
    
    return results


# Helper function to show entity changes
def show_entity_changes(filename):
    """Show what entities were changed in a specific file"""
    try:
        df = pd.read_csv(filename)
        
        if 'Entity' not in df.columns:
            print(f"‚ùå Error: 'Entity' column not found in {filename}")
            return
        
        print(f"\nüìä ENTITY ANALYSIS FOR {filename}:")
        print(f"{'='*50}")
        
        entity_counts = df['Entity'].value_counts()
        
        print(f"Top 10 most frequent entities:")
        for i, (entity, count) in enumerate(entity_counts.head(10).items(), 1):
            print(f"{i:>2}. {entity}: {count} mentions")
        
        print(f"\nTotal unique entities: {len(entity_counts)}")
        print(f"Total rows: {len(df)}")
        
    except FileNotFoundError:
        print(f"‚ùå Error: File '{filename}' not found.")
    except Exception as e:
        print(f"‚ùå Error analyzing {filename}: {e}")


# Main execution
if __name__ == "__main__":
    print("üöÄ ENTITY DATASET PROCESSING")
    print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Process the datasets
    results = process_entity_datasets()
    
    # Show analysis for processed files
    for filename in ['ner_entity_dataset_TOP_100.csv', 'ner_entity_dataset_TOP_100_UNIQUE.csv']:
        if filename in results and results[filename]['success']:
            show_entity_changes(filename)
    
    print(f"\n‚úÖ PROCESSING COMPLETED!")
    print(f"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
